# -*- coding: utf-8 -*-
"""Langchain_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LMw_V_xVvtux8nlk5mBEwXj4CyR7Js7L
"""

#!pip install -q langchain langchain-community langchain-openai langchain-chroma unstructured pypdf groq openai

#!pip install langchain_groq

import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
# Vector store with Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
# Build RAG chain
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq

from dotenv import load_dotenv
load_dotenv()

GROQ_API_KEY=os.getenv("GROQ_API_KEY")

pdf_path = "LLM.pdf"
loader = PyPDFLoader(pdf_path)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(documents)
print(splits[0])

embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Create vector store using Chroma
vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)
# Create retriever from the vector store
retriever = vectorstore.as_retriever()

#!pip install llama-cpp-python

llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.1,
    max_tokens=200,
    api_key=GROQ_API_KEY
)

from langchain.chains import RetrievalQA

from langchain.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("""
You are a helpful assistant with deep knowledge of LLMs.
Use the following context to answer the question concisely.

Context:
{context}

Question:
{question}
""")

rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# Ask questions
questions = [
    "What are the main differences between Foundation and Fine-Tuned LLMs?",
    "Summarize the evolution of LLMs.",
    "How can enterprises benefit from using LLMs?",
    "List the challenges of LLMs mentioned in the document.",
    "Explain the transformer architecture briefly."
]

# Run Q&A
for q in questions:
    result = rag_chain.invoke(q)
    print(f"\nðŸ§  Question: {q}\nðŸ“„ Answer: {result['result']}\n")